{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb68935-ccb1-4491-a61d-e6f2803d6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las liberías\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as pltimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as pltimg\n",
    "#import pydotplus\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix , ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df5370f",
   "metadata": {},
   "source": [
    "## PREPARACIÓN DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a34526b-79e5-4094-a5c2-3650979f8818",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Cargamos el csv\n",
    "df=pd.read_csv(\"cancer_mama.csv\",sep=\",\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691bb81a-2d21-4881-9876-a43795e0121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ver los valores de cada columna\n",
    "for i in df.columns:\n",
    "    print(df[i].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f05974-48b9-43f5-af55-4882347d6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vemos los valores que tiene cada columna y cuantas veces aparece cada uno\n",
    "#de ellos\n",
    "for i in df.columns:\n",
    "    print(i,df[i].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb12a4d-4f35-4d4b-b6a5-207d0275702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Información del dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3746229a-97f2-4022-acf5-5542d253b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos el dataset como csv\n",
    "df.to_csv(\"cancer_mama_py.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c58290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula la frecuencia de los valores de clase en todo el DataFrame\n",
    "frecuencia_clase = df['diagnosis'].value_counts()\n",
    "\n",
    "# Crea un gráfico de barras para visualizar la frecuencia de los valores \n",
    "#de clase\n",
    "plt.bar(frecuencia_clase.index, frecuencia_clase.values, color=\"pink\")\n",
    "plt.xlabel('Diagnóstico')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996961e7",
   "metadata": {},
   "source": [
    "# PREPROCESAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac6f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir diagnosis a numérico\n",
    "df.diagnosis = df.diagnosis.replace({\"B\": 0, \"M\": 1})\n",
    "\n",
    "# Dividimos entrada y salida\n",
    "x=df.iloc[:,2:32]\n",
    "print(x)\n",
    "y=df[[\"diagnosis\"]]\n",
    "# Dividimos en datos de entrenamiento y de validación\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Tamaños\n",
    "x.shape, y.shape\n",
    "\n",
    "x_train.shape, y_train.shape\n",
    "\n",
    "x_test.shape, y_test.shape\n",
    "\n",
    "# Calculamos el máximo y mínimo\n",
    "maxs = np.max(x_train, axis=0)\n",
    "mins = np.min(x_train, axis=0)\n",
    "\n",
    "ranges = maxs - mins\n",
    "\n",
    "# Normalizamos las variables\n",
    "x_train = (x_train - mins) / ranges\n",
    "x_test = (x_test - mins) / ranges\n",
    "\n",
    "# Calcula las frecuencias de clase en y_train\n",
    "class_frequencies = y_train.value_counts()\n",
    "\n",
    "# Calcula los pesos para cada clase  \n",
    "class_weights = {cls: 1.0 / freq for cls, freq in class_frequencies.items()}\n",
    "class_weights_int = {int(cls[0]): weight for cls, weight in class_weights.items()}\n",
    "\n",
    "print(class_frequencies)\n",
    "print(class_weights_int)\n",
    "\n",
    "pesos_ordenados=[]\n",
    "for i in range(2):\n",
    "    peso=class_weights_int[i]\n",
    "    pesos_ordenados.append(peso)\n",
    "\n",
    "print(pesos_ordenados)\n",
    "print(class_frequencies.values * pesos_ordenados)\n",
    "# Crea un gráfico de barras para visualizar la frecuencia ponderada de los valores de clase\n",
    "plt.bar([0,1], class_frequencies.values * pesos_ordenados, color= \"pink\")\n",
    "plt.xlabel('Diagnóstico')\n",
    "plt.ylabel('Ponderado')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced3d58",
   "metadata": {},
   "source": [
    "# MODELOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2d0d82",
   "metadata": {},
   "source": [
    "## Árbol de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8625f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo genérico\n",
    "DT = DecisionTreeClassifier(class_weight=class_weights_int)\n",
    "# entrenamos el modelo\n",
    "DT.fit(x_train, y_train)\n",
    "# hacer predicciones\n",
    "pred_DT = DT.predict(x_test)\n",
    "# informe\n",
    "print(classification_report(y_test,pred_DT))\n",
    "# cross value\n",
    "kfold = StratifiedKFold(10)\n",
    "score = cross_val_score(\n",
    "    DT, x_train, y_train, cv=kfold, scoring=\"accuracy\"\n",
    ")\n",
    "DT_prec=score.mean()\n",
    "print(\"cross_value: \", DT_prec)\n",
    "# matriz de confusión\n",
    "mat_conf = confusion_matrix(y_test, pred_DT)\n",
    "ConfusionMatrixDisplay(mat_conf).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660f6486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver opciones con otros parametros\n",
    "DT_combinaciones={}\n",
    "# parametros\n",
    "splitter_ = [\"best\", \"random\"]\n",
    "criterion_ = [\"gini\", \"entropy\"]\n",
    "max_features_ = [None,\"sqrt\", \"log2\"]\n",
    "porc_ = [0.2, 0.3]\n",
    "# combinaciones\n",
    "for c in criterion_:\n",
    "    for m in max_features_:\n",
    "        for s in splitter_:\n",
    "            for p in [1, 2]:\n",
    "                vector_parametros=[c,m,s,porc_[p-1]]\n",
    "                # dividimos las instancias\n",
    "                x_train, x_test, y_train, y_test = train_test_split(\n",
    "                    x, y, test_size=porc_[p - 1], random_state=42, shuffle=True\n",
    "                )\n",
    "                # modelo\n",
    "                DT = DecisionTreeClassifier(criterion=c, max_features=m, splitter=s, class_weight=class_weights_int)\n",
    "                # entrenamos el modelo\n",
    "                DT.fit(x_train, y_train)\n",
    "                # hacer predicciones\n",
    "                predicciones_DT = DT.predict(x_test)\n",
    "                # calculamos la exactitud del modelo\n",
    "                acc_1 = accuracy_score(y_test, predicciones_DT)\n",
    "                # cross value\n",
    "                kfold = StratifiedKFold(10)\n",
    "                score = cross_val_score(DT, x_train, y_train, cv=kfold, scoring=\"accuracy\")\n",
    "                print(\n",
    "                    c,\n",
    "                    \",\",\n",
    "                    m,\n",
    "                    \"y\",\n",
    "                    s,\n",
    "                    \"(\",\n",
    "                    porc_[p - 1],\n",
    "                    \")\",\n",
    "                    \"-> accuracy_score: \",\n",
    "                    acc_1,\n",
    "                    \"//\",\n",
    "                    \"cross_value: \",\n",
    "                    score.mean()\n",
    "                )\n",
    "                # añadimos al diccionario esta combinación\n",
    "                DT_combinaciones[score.mean()]=vector_parametros\n",
    "                \n",
    "# calculamos la precisión máxima\n",
    "DT_max_prec = max(DT_combinaciones)\n",
    "# miramos con que parámetros se consigue esta precisión\n",
    "DT_mejor_vector_parametros = DT_combinaciones[DT_max_prec]\n",
    "print(\"mejores resultados con los parametros -> \",DT_mejor_vector_parametros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3bda3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mejor modelo (utilizando el vector conseguido anteriormente)\n",
    "DT_mejor = DecisionTreeClassifier(criterion=DT_mejor_vector_parametros[0], max_features=DT_mejor_vector_parametros[1], splitter=DT_mejor_vector_parametros[2], class_weight=class_weights_int)\n",
    "# dividimos las instancias \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=DT_mejor_vector_parametros[3], random_state=42, shuffle=True)\n",
    "# entrenamos el modelo\n",
    "DT_mejor.fit(x_train, y_train)\n",
    "# hacer predicciones\n",
    "pred_DT_mejor = DT_mejor.predict(x_test)\n",
    "# informe\n",
    "print(classification_report(y_test,pred_DT_mejor))\n",
    "# cross value\n",
    "kfold = StratifiedKFold(10)\n",
    "score = cross_val_score(\n",
    "    DT_mejor, x_train, y_train, cv=kfold, scoring=\"accuracy\"\n",
    ")\n",
    "DT_mejor_prec=score.mean()\n",
    "print(\"cross_value: \", DT_mejor_prec)\n",
    "# matriz de confusión\n",
    "mat_conf = confusion_matrix(y_test, pred_DT_mejor)\n",
    "ConfusionMatrixDisplay(mat_conf).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8169f5e",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a9b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo genérico\n",
    "y_ravel= np.ravel(y)\n",
    "RF = RandomForestClassifier(class_weight=class_weights_int)\n",
    "# entrenamos el modelo\n",
    "y_train=y_train.values.ravel()\n",
    "RF.fit(x_train, y_train)\n",
    "# hacer predicciones\n",
    "pred_RF = RF.predict(x_test)\n",
    "# informe\n",
    "print(classification_report(y_test,pred_RF,zero_division=0))\n",
    "# cross value\n",
    "kfold = StratifiedKFold(10)\n",
    "score = cross_val_score(\n",
    "    RF, x_train, y_train, cv=kfold, scoring=\"accuracy\"\n",
    ")\n",
    "RF_prec=score.mean()\n",
    "print(\"cross_value: \", RF_prec)\n",
    "# matriz de confusión\n",
    "mat_conf = confusion_matrix(y_test, pred_RF)\n",
    "ConfusionMatrixDisplay(mat_conf).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c1d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver opciones con otros parámetros\n",
    "RF_combinaciones={}\n",
    "# parametros\n",
    "criterion_ = [\"gini\", \"entropy\"]\n",
    "max_features_ = [None,\"sqrt\", \"log2\"]\n",
    "porc_ = [0.2, 0.3]\n",
    "# combinaciones\n",
    "for c in criterion_:\n",
    "    for m in max_features_:\n",
    "        for p in [1, 2]:\n",
    "            vector_parametros=[c,m,porc_[p-1]]\n",
    "            # dividimos las instancias\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=porc_[p - 1], random_state=42, shuffle=True)\n",
    "            y_train=y_train.values.ravel()\n",
    "            # modelo\n",
    "            RF = RandomForestClassifier(criterion=c, max_features=m, class_weight=class_weights_int)\n",
    "            # entrenamos el modelo\n",
    "            RF.fit(x_train, y_train)\n",
    "            # hacer predicciones\n",
    "            predicciones_RF = RF.predict(x_test)\n",
    "            # calculamos la exactitud del modelo\n",
    "            acc_1 = accuracy_score(y_test, predicciones_RF)\n",
    "            # cross value\n",
    "            kfold = StratifiedKFold(10)\n",
    "            score = cross_val_score(RF, x_train, y_train, cv=kfold, scoring=\"accuracy\")\n",
    "            print(\n",
    "                c,\n",
    "                \"y\",\n",
    "                m,\n",
    "                \"(\",\n",
    "                porc_[p - 1],\n",
    "                \")\",\n",
    "                \"-> accuracy_score: \",\n",
    "                acc_1,\n",
    "                \"//\",\n",
    "                \"cross_value: \",\n",
    "                score.mean()\n",
    "            )\n",
    "            # añadimos al diccionario esta combinación\n",
    "            RF_combinaciones[score.mean()]=vector_parametros\n",
    "                \n",
    "# calculamos la precisión máxima\n",
    "RF_max_prec = max(RF_combinaciones)\n",
    "# miramos con que parámetros se consigue esta precisión\n",
    "RF_mejor_vector_parametros = RF_combinaciones[RF_max_prec]\n",
    "print(\"mejores resultados con los parametros -> \",RF_mejor_vector_parametros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1252b350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mejor modelo (utilizando el vector conseguido anteriormente)\n",
    "RF_mejor = RandomForestClassifier(criterion=RF_mejor_vector_parametros[0], max_features=RF_mejor_vector_parametros[1], class_weight=class_weights_int)\n",
    "# dividimos las instancias \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=RF_mejor_vector_parametros[2], random_state=42, shuffle=True)\n",
    "y_train=y_train.values.ravel()\n",
    "# entrenamos el modelo\n",
    "RF_mejor.fit(x_train, y_train)\n",
    "# hacer predicciones\n",
    "pred_RF_mejor = RF_mejor.predict(x_test)\n",
    "# informe\n",
    "print(classification_report(y_test,pred_RF_mejor))\n",
    "# cross value\n",
    "kfold = StratifiedKFold(10)\n",
    "score = cross_val_score(\n",
    "    RF_mejor, x_train, y_train, cv=kfold, scoring=\"accuracy\"\n",
    ")\n",
    "RF_mejor_prec=score.mean()\n",
    "print(\"cross_value: \", RF_mejor_prec)\n",
    "# matriz de confusión\n",
    "mat_conf = confusion_matrix(y_test, pred_RF_mejor)\n",
    "ConfusionMatrixDisplay(mat_conf).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb053e7",
   "metadata": {},
   "source": [
    "## Redes neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764fce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = [\"adam\", \"SGD\", \"RMSprop\", \"adagrad\", \"adamax\"]\n",
    "RN_combinaciones = {}\n",
    "\n",
    "# Separa los atributos y la variable de salida\n",
    "x = df.iloc[:, 2:32].values  \n",
    "y = df.iloc[:, 1].values  \n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escala los atributos para normalizarlos\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# ESTRUCTURA 1\n",
    "RNmodel = Sequential()\n",
    "\n",
    "# Agrega capas densas (totalmente conectadas) al modelo\n",
    "RNmodel.add(Dense(64, activation='relu', input_shape=(30,)))  # Capa oculta 1\n",
    "RNmodel.add(Dense(64, activation='relu'))  # Capa oculta 2\n",
    "RNmodel.add(Dense(1, activation='sigmoid'))  # Capa de salida (1 nivel de salida)\n",
    "\n",
    "for i in optim:\n",
    "    # Compila el modelo\n",
    "    RNmodel.compile(optimizer=i, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Entrena el modelo\n",
    "    RNmodel.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "    y_pred_prob = RNmodel.predict(x_test)\n",
    "    y_pred = np.round(y_pred_prob)\n",
    "    y_pred = np.round(np.nan_to_num(y_pred))\n",
    "    \n",
    "    # Calcular precisión\n",
    "    precision = accuracy_score(y_test, y_pred)\n",
    "    print(\"Precisión:\", precision)\n",
    "\n",
    "    # Calcular matriz de confusión\n",
    "    matriz_confusion = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Matriz de confusión:\")\n",
    "    ConfusionMatrixDisplay(matriz_confusion).plot()\n",
    "    plt.show()\n",
    "    \n",
    "    RN_combinaciones[precision]=[1,i]\n",
    "    \n",
    "# ESTRUCTURA 2\n",
    "RNmodel = Sequential()\n",
    "\n",
    "# Agrega capas densas (totalmente conectadas) al modelo\n",
    "RNmodel.add(Dense(128, activation='relu', input_shape=(30,)))  # Capa oculta 1\n",
    "RNmodel.add(Dense(64, activation='relu'))  # Capa oculta 2\n",
    "RNmodel.add(Dense(32, activation='relu'))  # Capa oculta 3\n",
    "RNmodel.add(Dense(1, activation='sigmoid'))  # Capa de salida (1 nivel de salida)\n",
    "\n",
    "for i in optim:\n",
    "    # Compila el modelo\n",
    "    RNmodel.compile(optimizer=i, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Entrena el modelo\n",
    "    RNmodel.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "    y_pred_prob = RNmodel.predict(x_test)\n",
    "    y_pred = np.round(y_pred_prob)\n",
    "    y_pred = np.round(np.nan_to_num(y_pred))\n",
    "    \n",
    "    # Calcular precisión\n",
    "    precision = accuracy_score(y_test, y_pred)\n",
    "    print(\"Precisión:\", precision)\n",
    "\n",
    "    # Calcular matriz de confusión\n",
    "    matriz_confusion = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Matriz de confusión:\")\n",
    "    ConfusionMatrixDisplay(matriz_confusion).plot()\n",
    "    plt.show()\n",
    "    \n",
    "    RN_combinaciones[precision] = [2, i]\n",
    "    \n",
    "# ESTRUCTURA 3\n",
    "RNmodel = Sequential()\n",
    "\n",
    "# Agrega capas densas (totalmente conectadas) al modelo\n",
    "RNmodel.add(Dense(64, activation='relu', input_shape=(30,)))  # Capa oculta 1\n",
    "RNmodel.add(Dense(128, activation='relu'))  # Capa oculta 2\n",
    "RNmodel.add(Dense(64, activation='relu'))  # Capa oculta 3\n",
    "RNmodel.add(Dense(32, activation='relu'))  # Capa oculta 4\n",
    "RNmodel.add(Dense(1, activation='sigmoid'))  # Capa de salida (1 nivel de salida)\n",
    "\n",
    "for i in optim:\n",
    "    # Compila el modelo\n",
    "    RNmodel.compile(optimizer=i, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Entrena el modelo\n",
    "    RNmodel.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "    y_pred_prob = RNmodel.predict(x_test)\n",
    "    y_pred = np.round(y_pred_prob)\n",
    "    y_pred = np.round(np.nan_to_num(y_pred))\n",
    "  \n",
    "    # Calcular precisión\n",
    "    precision = accuracy_score(y_test, y_pred)\n",
    "    print(\"Precisión:\", precision)\n",
    "\n",
    "    # Calcular matriz de confusión\n",
    "    matriz_confusion = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Matriz de confusión:\")\n",
    "    ConfusionMatrixDisplay(matriz_confusion).plot()\n",
    "    plt.show()\n",
    "    \n",
    "    RN_combinaciones[precision] = [3, i]\n",
    "    \n",
    "print(RN_combinaciones)\n",
    "# Calculamos la precisión máxima\n",
    "RN_max_prec = max(RN_combinaciones)\n",
    "# Miramos con qué parámetros se consigue esta precisión\n",
    "RN_mejor_vector_parametros = RN_combinaciones[RN_max_prec]\n",
    "print(\"Mejores resultados con los parámetros:\", RN_mejor_vector_parametros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b278bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejor modelo\n",
    "\n",
    "RNmodel_mejor = Sequential()\n",
    "\n",
    "if RN_mejor_vector_parametros[0]==1:\n",
    "    RNmodel_mejor.add(Dense(64, activation='relu', input_shape=(30,)))  # Capa oculta 1\n",
    "    RNmodel_mejor.add(Dense(64, activation='relu'))  # Capa oculta 2\n",
    "    RNmodel_mejor.add(Dense(1, activation='sigmoid'))  # Capa de salida (1 nivel de salida)\n",
    "elif RN_mejor_vector_parametros[0]==2:\n",
    "    RNmodel_mejor.add(Dense(128, activation='relu', input_shape=(30,)))  # Capa oculta 1\n",
    "    RNmodel_mejor.add(Dense(64, activation='relu'))  # Capa oculta 2\n",
    "    RNmodel_mejor.add(Dense(32, activation='relu'))  # Capa oculta 3\n",
    "    RNmodel_mejor.add(Dense(1, activation='sigmoid'))  # Capa de salida (1 nivel de salida)\n",
    "elif RN_mejor_vector_parametros[0]==3:\n",
    "    RNmodel_mejor.add(Dense(64, activation='relu', input_shape=(30,)))  # Capa oculta 1\n",
    "    RNmodel_mejor.add(Dense(128, activation='relu'))  # Capa oculta 2\n",
    "    RNmodel_mejor.add(Dense(64, activation='relu'))  # Capa oculta 3\n",
    "    RNmodel_mejor.add(Dense(32, activation='relu'))  # Capa oculta 4\n",
    "    RNmodel_mejor.add(Dense(1, activation='sigmoid'))  # Capa de salida (1 nivel de salida)\n",
    "# Compila el modelo\n",
    "RNmodel_mejor.compile(optimizer=RN_mejor_vector_parametros[1], loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrena el modelo\n",
    "RNmodel_mejor.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "y_pred_prob = RNmodel_mejor.predict(x_test)\n",
    "y_pred = np.round(y_pred_prob)\n",
    "\n",
    "# Calcular precisión\n",
    "RN_mejor_prec = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisión:\", RN_mejor_prec)\n",
    "\n",
    "# Calcular matriz de confusión\n",
    "matriz_confusion = confusion_matrix(y_test, y_pred)\n",
    "print(\"Matriz de confusión:\")\n",
    "ConfusionMatrixDisplay(matriz_confusion).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d60cb",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754db31",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score=[]\n",
    "max_score=0\n",
    "best_k=0\n",
    "for k in range(3,30):\n",
    "    knn_model=KNeighborsClassifier(n_jobs=-1,n_neighbors=k)\n",
    "    score=cross_val_score(knn_model,x_train,y_train,n_jobs=-1,cv=5,scoring='accuracy')\n",
    "    if score.mean()>max_score:\n",
    "        max_score=score.mean()\n",
    "        best_k=k\n",
    "    avg_score.append(score.mean())\n",
    "\n",
    "print([best_k,max(avg_score)])\n",
    "\n",
    "max_index = avg_score.index(max(avg_score))\n",
    "max_value = max(avg_score)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(range(3, 30), avg_score, color='magenta', linewidth=2)  \n",
    "plt.scatter(max_index+2, max_value, color='purple', marker='o', s=200)  \n",
    "plt.xlabel(\"k neighbors\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e6f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo\n",
    "KNN=KNeighborsClassifier(n_neighbors=best_k)\n",
    "# entrenamos el modelo\n",
    "KNN.fit(x_train,y_train)\n",
    "# hacer predicciones\n",
    "pred_KNN = KNN.predict(x_test)\n",
    "# informe\n",
    "print(classification_report(y_test,pred_KNN,zero_division=0))\n",
    "# cross value\n",
    "kfold = StratifiedKFold(10)\n",
    "score = cross_val_score(\n",
    "    KNN, x_train, y_train, cv=kfold, scoring=\"accuracy\"\n",
    ")\n",
    "KNN_prec=score.mean()\n",
    "print(\"cross_value: \", KNN_prec)\n",
    "# matriz de confusión\n",
    "mat_conf = confusion_matrix(y_test, pred_KNN)\n",
    "ConfusionMatrixDisplay(mat_conf).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf07e7f",
   "metadata": {},
   "source": [
    "# SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1ac1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo\n",
    "modelsvc = SVC()\n",
    "# entrenamos el modelo\n",
    "modelsvc.fit(x_train, y_train)\n",
    "# hacer predicciones\n",
    "pred_SVC = modelsvc.predict(x_test)\n",
    "# informe\n",
    "print(classification_report(y_test,pred_SVC,zero_division=0))\n",
    "# cross value\n",
    "kfold = StratifiedKFold(10)\n",
    "score = cross_val_score(\n",
    "    modelsvc, x_train, y_train, cv=kfold, scoring=\"accuracy\"\n",
    ")\n",
    "SVC_prec=score.mean()\n",
    "print(\"cross_value: \", SVC_prec)\n",
    "# matriz de confusión\n",
    "mat_conf = confusion_matrix(y_test, pred_SVC)\n",
    "ConfusionMatrixDisplay(mat_conf).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e1d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver opciones con otros parámetros\n",
    "svc_combinaciones={}\n",
    "# parametros\n",
    "c_ = [1, 20, 100, 200]\n",
    "kernel_ = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "porc_ = [0.2, 0.3]\n",
    "# combinaciones\n",
    "for c in c_:\n",
    "    for k in kernel_:\n",
    "        for p in [1, 2]:\n",
    "            vector_parametros=[c,k,porc_[p-1]]\n",
    "            # dividimos las instancias\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=porc_[p - 1], random_state=42, shuffle=True)\n",
    "            # modelo\n",
    "            modelsvc = SVC(C=c, kernel=k, class_weight=class_weights_int)\n",
    "            # entrenamos el modelo\n",
    "            modelsvc.fit(x_train, y_train)\n",
    "            # hacer predicciones\n",
    "            predicciones_SVC = modelsvc.predict(x_test)\n",
    "            # calculamos la exactitud del modelo\n",
    "            acc_1 = accuracy_score(y_test, predicciones_SVC)\n",
    "            # cross value\n",
    "            kfold = StratifiedKFold(10)\n",
    "            score = cross_val_score(modelsvc, x_train, y_train, cv=kfold, scoring=\"accuracy\")\n",
    "            print(\n",
    "                c,\n",
    "                \"y\",\n",
    "                k,\n",
    "                \"(\",\n",
    "                porc_[p - 1],\n",
    "                \")\",\n",
    "                \"-> accuracy_score: \",\n",
    "                acc_1,\n",
    "                \"//\",\n",
    "                \"cross_value: \",\n",
    "                score.mean()\n",
    "            )\n",
    "            # añadimos al diccionario esta combinación\n",
    "            svc_combinaciones[score.mean()]=vector_parametros\n",
    "                \n",
    "# calculamos la precisión máxima\n",
    "svc_max_prec = max(svc_combinaciones)\n",
    "# miramos con que parámetros se consigue esta precisión\n",
    "svc_mejor_vector_parametros = svc_combinaciones[svc_max_prec]\n",
    "print(\"mejores resultados con los parametros -> \",svc_mejor_vector_parametros) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c45704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mejor modelo (utilizando el vector conseguido anteriormente)\n",
    "svc_mejor = SVC(C=svc_mejor_vector_parametros[0], kernel=svc_mejor_vector_parametros[1], class_weight=class_weights_int)\n",
    "# dividimos las instancias \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=svc_mejor_vector_parametros[2], random_state=42, shuffle=True)\n",
    "# entrenamos el modelo\n",
    "svc_mejor.fit(x_train, y_train)\n",
    "# hacer predicciones\n",
    "pred_svc_mejor = svc_mejor.predict(x_test)\n",
    "# informe\n",
    "print(classification_report(y_test,pred_svc_mejor,zero_division=1))\n",
    "# cross value\n",
    "kfold = StratifiedKFold(10)\n",
    "score = cross_val_score(\n",
    "    svc_mejor, x_train, y_train, cv=kfold, scoring=\"accuracy\"\n",
    ")\n",
    "svc_mejor_prec=score.mean()\n",
    "print(\"cross_value: \", svc_mejor_prec)\n",
    "# matriz de confusión\n",
    "mat_conf = confusion_matrix(y_test, pred_svc_mejor)\n",
    "ConfusionMatrixDisplay(mat_conf).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42576ad2",
   "metadata": {},
   "source": [
    "# DIAGNOSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96506c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_v=str(input(\"Ahora introduciras los datos médicos del paciente con nombre o identificador: \"))\n",
    "radius_mean_v=float(input(\"radio (media): \"))\n",
    "texture_mean_v=float(input(\"textura (media): \"))\n",
    "perimeter_mean_v=float(input(\"perimetro (media): \"))\n",
    "area_mean_v=float(input(\"area (media): \"))\n",
    "smoothness_mean_v=float(input(\"suavidad (media): \"))\n",
    "compactness_mean_v=float(input(\"compacidad (media): \"))\n",
    "concavity_mean_v=float(input(\"concavidad (media): \"))\n",
    "concave_points_mean_v=float(input(\"puntos cóncavos (media): \"))\n",
    "symmetry_mean_v=float(input(\"simetría (media): \"))\n",
    "fractal_dimension_mean_v=float(input(\"dimensión fractal (media): \"))\n",
    "radius_se_v=float(input(\"radio (se): \"))\n",
    "texture_se_v=float(input(\"textura (se): \"))\n",
    "perimeter_se_v=float(input(\"perimetro (se): \"))\n",
    "area_se_v=float(input(\"area (se): \"))\n",
    "smoothness_se_v=float(input(\"suavidad (se): \"))\n",
    "compactness_se_v=float(input(\"compacidad (se): \"))\n",
    "concavity_se_v=float(input(\"concavidad (se): \"))\n",
    "concave_points_se_v=float(input(\"puntos cóncavos (se): \"))\n",
    "symmetry_se_v=float(input(\"simetría (se): \"))\n",
    "fractal_dimension_se_v=float(input(\"dimensión fractal (se): \"))\n",
    "radius_worst_v=float(input(\"radio (worst): \"))\n",
    "texture_worst_v=float(input(\"textura (worst): \"))\n",
    "perimeter_worst_v=float(input(\"perimetro (worst): \"))\n",
    "area_worst_v=float(input(\"area (worst): \"))\n",
    "smoothness_worst_v=float(input(\"suavidad (worst): \"))\n",
    "compactness_worst_v=float(input(\"compacidad (worst): \"))\n",
    "concavity_worst_v=float(input(\"concavidad (worst): \"))\n",
    "concave_points_worst_v=float(input(\"puntos cóncavos (worst): \"))\n",
    "symmetry_worst_v=float(input(\"simetría (worst): \"))\n",
    "fractal_dimension_worst_v=float(input(\"dimensión fractal (worst): \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8f3a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pac=[radius_mean_v, texture_mean_v, perimeter_mean_v, area_mean_v, smoothness_mean_v, compactness_mean_v, concavity_mean_v, concave_points_mean_v, symmetry_mean_v, fractal_dimension_mean_v, radius_se_v, texture_se_v, perimeter_se_v, area_se_v, smoothness_se_v, compactness_se_v, concavity_se_v, concave_points_se_v, symmetry_se_v, fractal_dimension_se_v, radius_worst_v, texture_worst_v, perimeter_worst_v, area_worst_v, smoothness_worst_v, compactness_worst_v, concavity_worst_v, concave_points_worst_v, symmetry_worst_v, fractal_dimension_worst_v]\n",
    "#pac=[17.99,10.38,122.8,1001.0,0.1184,0.2776,0.3001,0.1471,0.2419,0.07871,1.095,0.9053,8.589,153.4,0.006399,0.04904,0.05373,0.01587,0.03003,0.006193,25.38,17.33,184.6,2019.0,0.1622,0.6656,0.7119,0.2654,0.4601,0.1189]\n",
    "#nombre_v=\"magali\"\n",
    "\n",
    "# Crear los DataFrames que necesitaremos\n",
    "df_diagnostico = pd.DataFrame(columns=['Método', 'Características', 'Precisión', 'Diagnóstico'])\n",
    "\n",
    "df_datos=pd.DataFrame({'medida':['radio(media)', 'textura(media)', 'perimetro(media)', 'area(media)', 'suavidad(media)', 'compacidad(media)', 'concavidad(media)', 'puntos_concavos(media)', 'simetria(media)', 'dimesion_fractal(media)', 'radio(se)', 'textura(se)', 'perimetro(se)', 'area(se)', 'suavidad(se)', 'compacidad(se)', 'concavidad(se)', 'puntos_concavos(se)', 'simetria(se)', 'dimesion_fractal(se)','radio(worst)', 'textura(worst)', 'perimetro(worst)', 'area(worst)', 'suavidad(worst)', 'compacidad(worst)', 'concavidad(worst)', 'puntos_concavos(worst)', 'simetria(worst)', 'dimesion_fractal(worst)'],\n",
    "                     'valor':pac})\n",
    "\n",
    "df_nombre=pd.DataFrame({'nombre':[nombre_v]})\n",
    "\n",
    "# Predecir diagnóstico con diferentes métodos\n",
    "DT_predict=int(DT_mejor.predict([pac]))\n",
    "RF_predict=int(RF_mejor.predict([pac]))\n",
    "pac_scaled = scaler.transform([pac])  # Escala el vector pac\n",
    "y_pred_prob = RNmodel_mejor.predict(pac_scaled)\n",
    "RN_predict = np.argmax(y_pred_prob, axis=1)\n",
    "KNN_predict=int(KNN.predict([pac]))\n",
    "svc_predict=int(svc_mejor.predict([pac]))\n",
    "\n",
    "df_diagnostico = df_diagnostico.append({'Método': 'Árbol de decisión', 'Características': DT_mejor_vector_parametros, 'Precisión': DT_mejor_prec, 'Diagnóstico': DT_predict}, ignore_index=True)\n",
    "df_diagnostico = df_diagnostico.append({'Método': 'Random Forest', 'Características': RF_mejor_vector_parametros, 'Precisión': RF_mejor_prec, 'Diagnóstico': RF_predict}, ignore_index=True)\n",
    "df_diagnostico = df_diagnostico.append({'Método': 'Redes neuronales', 'Características': RN_mejor_vector_parametros, 'Precisión': RN_mejor_prec, 'Diagnóstico': RN_predict[0]}, ignore_index=True)\n",
    "df_diagnostico = df_diagnostico.append({'Método': 'KNN', 'Características': 'K='+ str(best_k), 'Precisión': KNN_prec, 'Diagnóstico': KNN_predict}, ignore_index=True)\n",
    "df_diagnostico = df_diagnostico.append({'Método': 'SVC', 'Características': svc_mejor_vector_parametros, 'Precisión': svc_mejor_prec, 'Diagnóstico': svc_predict}, ignore_index=True)\n",
    "\n",
    "df_diagnostico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479b2d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos los dataset como csv\n",
    "df_datos.to_csv(\"datospac_cancer_mama_py.csv\", index=False)\n",
    "df_diagnostico.to_csv(\"diagnostico_cancer_mama_py.csv\", index=False)\n",
    "df_nombre.to_csv(\"nombrepac_cancer_mama_py.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ee68e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
